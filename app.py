import os
from dotenv import load_dotenv
import asyncio
from flask import Flask, jsonify, request 

load_dotenv()  # .env の読み込み
app = Flask(__name__)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SHEET_ID = os.getenv("SHEET_ID")
CSV_URL = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/gviz/tq?tqx=out:csv"


import faiss
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import asyncio

# 公開された Google スプレッドシートの 読み込み
df = pd.read_csv(CSV_URL)
print("Googleスプレッドシートからデータを取得しました！")

# Embedding　モデルのロード（検索用）
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
# T5 (生成) モデルのロード
tokenizer_t5 = T5Tokenizer.from_pretrained("t5-small")
model_t5 = T5ForConditionalGeneration.from_pretrained("t5-small")

# データベースを作成
docs = df.apply(lambda row: ",  ".join(f"{col}: {val}" for col, val in zip(df.columns, row)), axis=1).tolist()
'''
# 埋め込み生成
doc_embeddings = embedding_model.encode(docs, batch_size=64, show_progress_bar=True)
'''
# Railway のボリュームに保存したファイルのパス
FILE_PATH = "./doc_embeddings.npy"
# ファイルが存在するか確認
if os.path.exists(FILE_PATH):
    doc_embeddings = np.load(FILE_PATH)
    print("doc_embeddings.npy をロードしました！")
else:
    print("エラー: doc_embeddings.npy が見つかりません！")

print("T5モデルのウォームアップ開始！")
#T5モデルを事前にウォームアップする(T5モデルの初回遅延 (Lazy Initialization)対策)
dummy_text = "This is test document."
inputs = tokenizer_t5(dummy_text, return_tensors="pt", padding=True, truncation=True, max_length=256)
with torch.no_grad():
    model_t5.generate(
        **inputs,
        min_length=10,
        max_length=50,
        num_beams=2,
        no_repeat_ngram_size=2,
        early_stopping=True 
    )
print("✅ T5モデルのウォームアップ完了！")


import openai
from openai import AsyncOpenAI
client = AsyncOpenAI(api_key=OPENAI_API_KEY)
import time

# Case 1: OpenAI を使った ０ からの生成関数
async def generate_story(prompt, model="gpt-3.5-turbo", max_tokens=300):
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens, 
            temperature=0.8,
        )
        # Access the content in the latest response format
        story = response.choices[0].message.content
        return story
        
    except Exception as e:
        return f"An error occurred: {e}"


# Case 2: T5 による新しい Summary 生成関数
def generate_summary_from_multiple_docs(input_doc, prefix="create a coherent story summary: "):
    print ("呼んだ？呼んだよね今")
    combined_text = " ".join(input_doc)
    input_text = prefix + combined_text
    inputs = tokenizer_t5(input_text, return_tensors="pt", padding=True, truncation=True, max_length=256)
    start = time.time()
    
    with torch.no_grad():
        output_ids = model_t5.generate(
            **inputs,
            min_length=100,
            max_length=300,
            num_beams=5,
            no_repeat_ngram_size=2,
            early_stopping=False
        )

    if output_ids is None or len(output_ids) == 0:
        print("Error: No output generated by T5 model")
    else :
        print("output done!")
    end = time.time()
    T5_gene = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)
    print(end - start)
    return T5_gene

# OpenAI API を使って Summary を自然な文章にする関数
async def refine_summary_with_openai(summary):
    response = await client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an expert at writing natural and engaging summaries."},
            {"role": "user", "content": f"Please refine the following summary to make it more natural and engaging:\n\n{summary}"}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content
    print("AI done!!")


# ユーザーの質問を受け取る
def process_query(query, TARGET_SIMILARITY, SIMILARITY_THRESHOLD):
    #query_embedding = embedding_model.encode([query])
    query_embedding = np.array(embedding_model.encode([query])).astype('float32')
    # FAISS ベクトル検索エンジンを構築
    dimension = doc_embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(doc_embeddings)
    print("FAISS ベクトル検索エンジンを構築")
    # FAISS を使って類似文書を検索 (上位5件)
    D, I = index.search(query_embedding, k=5)

    # コサイン類似度を計算
    query_vector = query_embedding / np.linalg.norm(query_embedding)  # 正規化
    doc_vectors = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)  # 正規化
    similarities = cosine_similarity(query_vector, doc_vectors)[0]

    # ターゲット類似度に最も近い文書を取得
    closest_docs = [(docs[i], similarities[i]) for i in range(len(docs))]
    sorted_docs = sorted(closest_docs, key=lambda x: abs(x[1] - TARGET_SIMILARITY))[:5]

    summaries = []
    for doc, sim in sorted_docs:
        doc_data = dict(item.split(": ", 1) for item in doc.split(",  ") if ": " in item)
        if abs(sim - TARGET_SIMILARITY) <= SIMILARITY_THRESHOLD:
            title_text = doc_data.get("title", "No title available")
            genre_text = doc_data.get("genre", "No genre available")
            summary_text = doc_data.get("summary", "No summary available")
            summaries.append(summary_text)

    # Switch はここで
    if not summaries:
        print("該当なし (新しい Summary を生成します)")
        ai_answer = asyncio.run(generate_story(query))
        return ai_answer
        #return jsonify({"ナッシング！！": ai_answer})
    else:
        print("近似 5 件の類似 Summary を元に新しい Summary を生成しました")
        T5_answer = generate_summary_from_multiple_docs(summaries)
        ai_answer = asyncio.run(refine_summary_with_openai(T5_answer))
        return ai_answer
        #return jsonify({"great": ai_answer})
        

@app.route("/", methods=["GET"])
def get_summary(): 
    query = request.args.get("query", default="genre: fantasy, summary: A young girl, Miu starts school and meets a special friend.")
    TARGET_SIMILARITY = float(request.args.get("TARGET_SIMILARITY", 0.4))
    SIMILARITY_THRESHOLD = float(request.args.get("SIMILARITY_THRESHOLD", 0.1))

    ai_answer = process_query(query,TARGET_SIMILARITY,SIMILARITY_THRESHOLD)
    return jsonify({"query": query, "\n\nresult": ai_answer})
    #return jsonify({"result": ai_answer})

'''
def run_async_function(async_func, *args):
    try:
        loop = asyncio.get_running_loop()
        future = asyncio.ensure_future(async_func(*args))
        return loop.run_until_complete(future)
    except RuntimeError:
        return asyncio.run(async_func(*args))
    ai_answer = run_async_function(refine_summary_with_openai, T5_answer)
'''

@app.route("/summary")
def index():
    return jsonify({"message": "Welcome to the Plot Generation API!"})

if __name__ == "__main__":
    from waitress import serve
    port = int(os.getenv("PORT", 8080))
    serve(app, host="0.0.0.0", port=port)
    #app.run(host="0.0.0.0", port=port)
